{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install openai-agents SDK**"
      ],
      "metadata": {
        "id": "fyFAmeE1vQP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKJFINniu0Pl",
        "outputId": "bc005ec2-7c23-4cc1-95ea-3e7ecc9472de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents  \"openai-agents[litellm]\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make your Jupyter Notebook capable of running asynchronous functions.**"
      ],
      "metadata": {
        "id": "H0Jn0X7IvYvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "rU_nM__lvk2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Google Gemini with LiteLLm and OPENAI-Agent SDK**"
      ],
      "metadata": {
        "id": "6s4wfUqWwq2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function Run Sync"
      ],
      "metadata": {
        "id": "bWppZV2owwxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "os.environ['GEMINI_API_KEY'] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You only respond in haikus.\",\n",
        "    # model=LitellmModel(model=MODEL, api_key=api_key),\n",
        "    model=LitellmModel(model=MODEL),\n",
        "\n",
        ")\n",
        "\n",
        "result = Runner.run_sync(agent, \"What's an agentic AI?\")\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PttnQtVNw4e_",
        "outputId": "94f2f625-021a-42fe-c873-95eae16bc072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI takes the lead,\n",
            "Setting goals, acting freely,\n",
            "A digital mind.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Async Function**"
      ],
      "metadata": {
        "id": "uT3FkrvURC2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str)->str:\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"WeatherAssistant\",\n",
        "    instructions=\"You will answer weather relevent questions only karachi city\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    tools=[get_weather],\n",
        "    handoff_description=\"Weather Assistant is specialized for All Weather Queries\"\n",
        ")\n",
        "\n",
        "giaic_agent = Agent(\n",
        "    name=\"GIAIC Assistant\",\n",
        "    instructions=\"You will answer GIAIC relevent questions, GIAIC is the governor iniative for artificial intelligenne and cloud computing\",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    handoff_description=\"GIAIC Assistant is trained to answer questions only related to the Governor Initiative for Artificial Intelligence and Cloud Computing (GIAIC)\"\n",
        "    \"including its courses, instructors, assignments, and events.\"\n",
        ")\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name=\"GeneralAssistant\",\n",
        "    instructions=\"You will chat with user for general questions and handoff to Specialized Assistants when needed i.e: WeatherAssistant for weather queries and GIAIC Assistant for All Governor Initiative for Artificial Intelligence and Cloud Computing \",\n",
        "    model=LitellmModel(model=MODEL, api_key=GEMINI_API_KEY),\n",
        "    handoffs=[weather_agent, giaic_agent]\n",
        ")\n",
        "\n",
        "result = await Runner.run(triage_agent, \"Hi\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55MCR9xtRF3A",
        "outputId": "528a4908-fc08-4832-e8fe-9df44c075922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi, how can I help you today?\n",
            "\n"
          ]
        }
      ]
    }
  ]
}